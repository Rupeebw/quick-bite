<details>
<summary>Introduction</summary>
<br>

  <img width="583" alt="image" src="https://user-images.githubusercontent.com/75510135/167283228-48fa1380-e40d-4e22-82ba-7a13fe3d0a97.png">
Introduction#

Now you’ll learn how to deploy a working StatefulSet. The example is intended to demonstrate the way StatefulSets work and reinforce what you’ve already learned. It’s not intended as a production-grade application configuration.

The examples cited will work on Kubernetes clusters running on GCP and GKE. The course’s GitHub repo contains YAML files for other cloud platforms.

If you’re following along, you’ll deploy the following three objects:

    A StorageClass
    A headless Service
    A StatefulSet

To make things easier to follow, you’ll inspect and deploy each object individually. However, all three objects can be grouped in a single YAML file, separated by three dashes. See the app.yml file below.

  ```
  apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: flash
provisioner: pd.csi.storage.gke.io
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
parameters:
  type: pd-ssd
---
# Headless Service for StatefulSet Pod DNS names
apiVersion: v1
kind: Service
metadata:
  name: tkb-sts
  labels:
    app: web
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: web
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: tkb-sts
spec:
  replicas: 3 
  selector:
    matchLabels:
      app: web
  serviceName: "tkb-sts"
  template:
    metadata:
      labels:
        app: web
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: ctr-web
        image: nginx:latest
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "flash"
      resources:
        requests:
          storage: 1Gi
  ```
  
  Deploying the StorageClass
  StatefulSets that use volumes need to be able to create them dynamically. You need two objects to do this:

    StorageClass (SC)
    PersistentVolumeClaim (PVC)

The following YAML is from the gcp-sc.yml file and defines a StorageClass object called flash, that will dynamically provision SSD volumes (type=pd-ssd) from GCP using the GCP persistent disk CSI driver (pd.csi.storage.gke.io). It will only work on Kubernetes clusters running on GCP or GKE with the CSI driver enabled. YAML files for creating StorageClasses on other cloud platforms, including AWS, Azure, and Linode, are also provided in the repo.
  
  ```
  apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: flash
provisioner: pd.csi.storage.gke.io
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
parameters:
  type: pd-ssd
  ```
  
  <img width="904" alt="image" src="https://user-images.githubusercontent.com/75510135/167283426-76fc4622-a6b7-46cc-951b-d73c54fdf809.png">

  
</details>

<details>
<summary>Creating a Governing Headless Service</summary>
<br>

  When learning about headless Services, it can be useful to visualize a Service object with a head and a tail. The head is the stable IP exposed on the network, and the tail is the list of Pods it will send traffic to. Therefore, a headless Service is a Service object without a ClusterIP.
Example#

The following YAML is the headless-svc.yml file and describes a headless Service, called dullahan, with no IP address (spec.clusterIP: None).

  ```
  apiVersion: v1
kind: Service
metadata:
  name: dullahan
  labels:
    app: web
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: web
  ```
  The only difference to a regular Service is that a headless Service must set the value clusterIP to None.

When combined with a StatefulSet, headless Services create predictable stable DNS entries for every Pod that matches the StatefulSets label selector. You’ll see this in a later step.

Deploy the headless Service to your cluster.

  <img width="688" alt="image" src="https://user-images.githubusercontent.com/75510135/167283540-7a883695-eb90-4a6a-8c46-2583817540af.png">

  
</details>

<details>
<summary>Deploy the StatefulSet</summary>
<br>

  <img width="548" alt="image" src="https://user-images.githubusercontent.com/75510135/167283572-55d636df-42d7-4083-8352-a739a093eeb8.png">

  With the StorageClass and headless Service in place, it’s time to deploy the StatefulSet.

The following YAML is the sts.yml file and defines the StatefulSet. Remember: this is for learning purposes only; it’s not intended as a production-grade deployment of an application.

  ```
  apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: tkb-sts
spec:
  replicas: 3 
  selector:
    matchLabels:
      app: web
  serviceName: "dullahan"
  template:
    metadata:
      labels:
        app: web
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: ctr-web
        image: nginx:latest
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: webroot
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: webroot
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "flash"
      resources:
        requests:
          storage: 1Gi
  ```
  
  <img width="883" alt="image" src="https://user-images.githubusercontent.com/75510135/167283783-2cab7c27-5fd4-4d65-844d-0dfa57fd508a.png">

  <img width="903" alt="image" src="https://user-images.githubusercontent.com/75510135/167283789-06b2240b-f194-487f-a695-34a0d6e68272.png">

  <img width="957" alt="image" src="https://user-images.githubusercontent.com/75510135/167283796-534189fb-f53d-4bf7-a5ac-e5da95a6dd4d.png">

  <img width="920" alt="image" src="https://user-images.githubusercontent.com/75510135/167283804-2457b0e8-ec44-4785-b439-9c5f1637783b.png">

  <img width="895" alt="image" src="https://user-images.githubusercontent.com/75510135/167283813-05c72353-1b7c-4e49-9a9c-d84e18ab32ae.png">

  
</details>

<details>
<summary>Testing Peer Discovery</summary>
<br>

  You know that pairing a headless Service with a StatefulSet creates DNS SRV records for each StatefulSet Pod that matches the Service’s label selector. You already have a headless Service and 3 StatefulSet Pods running, so you should have three DNS SRV records – one for each Pod.
How DNS hostnames subdomains work with StatefulSets#

By default, Kubernetes places all objects within the cluster.local DNS subdomain. You can choose something different, but most lab environments use this domain, so we’ll assume it in this example. Within that domain, Kubernetes constructs DNS subdomains as follows:

<object-name>.<service-name>.<namespace>.svc.cluster.local

    svc indicates the subdomain for objects behind a Service.

So far, you’ve got three Pods, called tkb-sts-0, tkb-sts-1, and tkb-sts-2, governed by the dullahan headless Service. This means the 3 Pods will have the following fully qualified DNS names:

    tkb-sts-0.dullahan.default.svc.cluster.local
    tkb-sts-1.dullahan.default.svc.cluster.local
    tkb-sts-2.dullahan.default.svc.cluster.local

To test this, you’ll deploy a jump-pod that has the DNS dig utility pre-installed. You’ll exec onto that Pod, and you’ll use dig to query DNS for SRV records in the dullahan.default.svc.cluster.local subdomain.

The following YAML is the jump-pod.yml file.

  ```
  apiVersion: v1
kind: Pod
metadata:
  name: jump-pod
spec:
  terminationGracePeriodSeconds: 1
  containers:
  - image: nigelpoulton/curl:1.0
    name: jump-ctr
    tty: true
    stdin: true
  ```
  
  <img width="904" alt="image" src="https://user-images.githubusercontent.com/75510135/167284174-199d6e28-71bd-4b54-bb91-7d11cfb8dd72.png">

  <img width="915" alt="image" src="https://user-images.githubusercontent.com/75510135/167284187-4f1b49d3-8df5-40b5-b90a-8f473c571ca1.png">

  
</details>

<details>
<summary>Scaling StatefulSets</summary>
<br>

  Each time a StatefulSet is scaled up, a Pod and a PVC is created. However, when scaling a StatefulSet down, the Pod is terminated but the PVC is not. This means future scale up operations only need to create a new Pod, which is then connected to the surviving PVC. The StatefulSet controller includes all of the intelligence to track and manage these mappings between Pods and PVCs.

You currently have 3 StatefulSet Pod replicas and 3 PVCs. Edit the sts.yml file and change the replica count from 3 down to 2 and save your change. When you’ve done that, run the following command to repost the YAML file to the cluster.

  <img width="917" alt="image" src="https://user-images.githubusercontent.com/75510135/167284389-98266d3f-99cc-4d34-8928-fa965b003a80.png">

  The fact that all three PVCs still exist means that scaling back up to three replicas only requires a new Pod to be created. As the name of the surviving PVC is webroot-tkb-sts-2, the StatefulSet controller knows to automatically connect it to the new Pod.

Edit the sts.yml file and increment the number of replicas back to 3 and save your change. When you’ve done that, repost the YAML file to the API server with the following command.

  <img width="904" alt="image" src="https://user-images.githubusercontent.com/75510135/167284403-f9e06414-ce5f-4c21-8b9b-456b4a1fd290.png">

  It’s worth noting that scale down operations will be put on hold if any of the Pods are in a failed state. This is to protect the resiliency of the app and integrity of its data.

Finally, it’s possible to tweak the controlled and ordered starting and stopping of Pods via the StatefulSet’s spec.podManagementPolicy property.

The default setting is OrderedReady and implements the strict methodical ordering previously explained. Setting the value to Parallel will cause the StatefulSet to act more like a Deployment, where Pods are created and deleted in parallel. For example, scaling from 2 > 5 Pods will create all three new Pods instantaneously, and scaling down from 5 > 2 will delete three Pods in parallel. StatefulSet naming rules are still implemented, and the setting only applies to scaling operations and does not impact rolling updates.
Performing rolling updates#

StatefulSets support rolling updates. You update the image version in the YAML and repost it to the API server. Once authenticated and authorized, the controller will replace old Pods with new ones. However, the process always starts with the highest numbered Pod and works down through the set, one at a time, until all Pods are on the new version. The controller also waits for each new Pod to be running and ready before replacing the one with the next lowest index ordinal.

For more information, run $ kubectl explain sts.spec.updateStrategy.

</details>
  
<details>
<summary> Performing rolling updates</summary>
<br>

 

StatefulSets support rolling updates. You update the image version in the YAML and repost it to the API server. Once authenticated and authorized, the controller will replace old Pods with new ones. However, the process always starts with the highest numbered Pod and works down through the set, one at a time, until all Pods are on the new version. The controller also waits for each new Pod to be running and ready before replacing the one with the next lowest index ordinal.

For more information, run $ kubectl explain sts.spec.updateStrategy.

</details>
  
<details>
<summary>Test a Pod Failure</summary>
<br>

  A simple way to test a failure is to manually delete a Pod. This will delete the Pod but not the associated PVC. The StatefulSet controller will notice the observed state varys from the desired state, realize that a Pod has been deleted, and start a new identical Pod in its place. This new Pod will have the same name and will be connected to the same PVC.

Let’s test it.

Confirm that you have three healthy Pods in your StatefulSet.

  <img width="660" alt="image" src="https://user-images.githubusercontent.com/75510135/167284793-092c759a-b60a-46ac-b0da-8c0545afaa1e.png">

  You’re about to delete the tkb-sts-0 Pod. But before you do that, let’s use $ kubectl describe to confirm the PVC it’s currently using. You don’t need to do this, as you can deduce the name of the PVC from the name of the volumeClaimTemplate and the StatefulSet. However, it’s good to confirm.
  
  <img width="896" alt="image" src="https://user-images.githubusercontent.com/75510135/167284800-95be0c67-d5fd-4dcc-89ae-359d563c475c.png">

  <img width="955" alt="image" src="https://user-images.githubusercontent.com/75510135/167284806-114e9169-46a2-4d7c-ad32-0eba0ee243c2.png">

  <img width="895" alt="image" src="https://user-images.githubusercontent.com/75510135/167284812-940edd3b-c5e5-4522-b969-c389889842ed.png">

  Deleting StatefulSets#

Earlier in the chapter, you learned that deleting a StatefulSet doesn’t terminate managed Pods in order. Therefore, if your application needs Pods shutting down in order, you should scale the StatefulSet to 0 replicas before initiating the delete operation.

Scale your StatefulSet to 0 replicas and confirm the operation. It may take a few seconds for the set to scale all the way down to 0.
  
  <img width="884" alt="image" src="https://user-images.githubusercontent.com/75510135/167284816-e73eb42a-540a-43f9-a69f-f0c1207898ce.png">

  
</details>
  
